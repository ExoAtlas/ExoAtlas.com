name: NAIF IDs â†’ GCS CSV + R2 JSON

on:
  schedule:
    - cron: "15 4 * * *"   # 04:15 UTC daily
  workflow_dispatch:

permissions:
  contents: read
  id-token: write   # needed for GCP Workload Identity Federation

concurrency:
  group: ${{ github.workflow }}
  cancel-in-progress: true

env:
  # --- Fetch controls (override in the workflow_dispatch inputs or repo env) ---
  SBDB_LIMIT: "10000"        # cap total rows pulled
  DAYS_LOOKBACK: "7"         # only objects numbered or updated in last N days (best-effort filter)
  INCLUDE_COMETS: "true"     # "true" or "false"
  # --- GCS destination ---
  GCS_BUCKET: "exoatlas-data"    
  GCS_OBJECT: "workflow/naif_ids.csv"
  GCS_CACHE_CONTROL: "public, max-age=3600, s-maxage=86400, immutable"
  # --- Cloudflare R2 destination ---   
  R2_OBJECT: "naif/naif_ids.json"
  R2_CACHE_CONTROL: "public, max-age=3600, s-maxage=86400, immutable"

jobs:
  export-naif-ids:
    runs-on: ubuntu-latest
    timeout-minutes: 20

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install packaging google-cloud-storage google-api-core psycopg2-binary boto3 botocore requests python-dateutil







      - name: 'Authenticate to Google Cloud'
        id: 'auth'
        uses: google-github-actions/auth@v2
        with:
          workload_identity_provider: 'projects/566178654810/locations/global/workloadIdentityPools/github-pool/providers/exoatlas'
          service_account: 'exoatlas-github-mpc-worker@exoatlas.iam.gserviceaccount.com'
          create_credentials_file: true
          export_environment_variables: true

      - name: Download Cloud SQL Auth Proxy v2
        env:
          PROXY_VERSION: v2.18.1
        run: |
          wget -q https://storage.googleapis.com/cloud-sql-connectors/cloud-sql-proxy/${PROXY_VERSION}/cloud-sql-proxy.linux.amd64 -O cloud-sql-proxy
          chmod +x cloud-sql-proxy

      - name: Start Cloud SQL Proxy
        env:
          INSTANCE_CONNECTION_NAME:  ${{ secrets.GOOGLE_CLOUDSQL_INSTANCE }}
          port: 5432
          quiet: true
        run: |
          set -euo pipefail
          ./cloud-sql-proxy --port 5432 "$INSTANCE_CONNECTION_NAME" > proxy.log 2>&1 &
          for i in {1..30}; do
            (echo > /dev/tcp/127.0.0.1/5432) >/dev/null 2>&1 && break
            sleep 1
          done
          (echo > /dev/tcp/127.0.0.1/5432) >/dev/null 2>&1 || { echo "Proxy failed to start"; tail -n +1 proxy.log; exit 1; }
    
      - name: Install psql client
        run: |
          sudo apt-get update -y && sudo apt-get install -y postgresql-client

      - name: Test Cloud PostgreSQL DB connection
        env:
          PGPASSWORD: ${{ secrets.DB_PASSWORD }}
        run: |
          psql "host=127.0.0.1 port=5432 user=${{ secrets.DB_USER }} dbname=${{ secrets.DB_NAME }} sslmode=disable" -c "select now();"

      - name: Run export script
        env:
          # Fetch controls
          SBDB_LIMIT: ${{ env.SBDB_LIMIT }}
          DAYS_LOOKBACK: ${{ env.DAYS_LOOKBACK }}
          INCLUDE_COMETS: ${{ env.INCLUDE_COMETS }}

          # GCS
          GCS_BUCKET: ${{ env.GCS_BUCKET }}
          GCS_OBJECT: ${{ env.GCS_OBJECT }}
          GCS_CACHE_CONTROL: ${{ env.GCS_CACHE_CONTROL }}
          GOOGLE_CLOUD_PROJECT: ${{ secrets.GCP_PROJECT_ID }}

          # Cloudflare R2 (S3-compatible)
          R2_ENDPOINT: ${{ secrets.R2_ENDPOINT }}
          R2_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
          R2_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
          R2_BUCKET: ${{ secrets.R2_BUCKET }}
          R2_OBJECT: ${{ env.R2_OBJECT }}
          R2_CACHE_CONTROL: ${{ env.R2_CACHE_CONTROL }}
        run: |
          python scripts/naif_id_export.py
