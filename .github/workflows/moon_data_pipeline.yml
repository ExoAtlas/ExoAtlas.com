name: Moons Data Update - Daily (NASA Horizons)

on:
  schedule:
    - cron: '0 1 * * *'   # 01:00 UTC daily
  workflow_dispatch:

env:
  PROJECT_ID: exoatlas
  INSTANCE_CONNECTION_NAME: exoatlas:us-central1:exoatlas-db

concurrency:
  group: ${{ github.workflow }}
  cancel-in-progress: true

jobs:
  fetch-per-planet:
    runs-on: ubuntu-latest
    timeout-minutes: 15
    permissions:
      contents: read
      id-token: write
    strategy:
      fail-fast: false
      matrix:
        planet: [earth, mars, jupiter, saturn, uranus, neptune, dwarf]

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          # put your deps in requirements.txt if possible
          pip install packaging google-cloud-storage google-api-core psycopg2-binary boto3 botocore requests

      - name: Fetch ${{ matrix.planet }} moons from NASA HORIZONS
        env:
          OUT_JSON: data/moons/moons_${{ matrix.planet }}.json
          OUT_CSV:  data/moons/moons_${{ matrix.planet }}.csv
          PLANET: ${{ matrix.planet }}
        run: |
          set -euo pipefail
          mkdir -p data/moons
          # Example: adapt your script to accept --planet / --out-json / --out-csv
          python ./backend/datapipe/moon_data_fetcher.py \
            --planet "$PLANET" \
            --out-json "$OUT_JSON" \
            --out-csv "$OUT_CSV"

      - name: Upload per-planet artifacts (always)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: moons-${{ matrix.planet }}
          path: |
            data/moons/moons_${{ matrix.planet }}.json
            data/moons/moons_${{ matrix.planet }}.csv

  build-aggregate-and-publish:
    needs: fetch-per-planet
    runs-on: ubuntu-latest
    timeout-minutes: 20
    permissions:
      contents: read
      id-token: write

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install packaging google-cloud-storage google-api-core psycopg2-binary boto3 botocore requests

      - name: Download all per-planet artifacts
        uses: actions/download-artifact@v4
        with:
          path: data/moons

      - name: Concatenate JSON & CSV
        run: |
          set -euo pipefail
          ls -R data/moons || true
          # Expect a structure like data/moons/moons-jupiter/... from download-artifact
          # Flatten and concat:
          mkdir -p build
          find data/moons -type f -name "moons_*.json" -exec cp {} build/ \;
          find data/moons -type f -name "moons_*.csv"  -exec cp {} build/ \;

          # Your concat script should read all moons_*.json/.csv in build/ and write the aggregate:
          python ./backend/datapipe/concat_moons.py \
            --in-dir build \
            --out-json build/moons_all.json \
            --out-csv  build/moons_all.csv

      # === Optional: upsert DB here (single place) ===
      - name: Download Cloud SQL Auth Proxy v2
        env:
          PROXY_VERSION: v2.18.1   # keep this pinned intentionally
        run: |
          wget -q https://storage.googleapis.com/cloud-sql-connectors/cloud-sql-proxy/${PROXY_VERSION}/cloud-sql-proxy.linux.amd64 -O cloud-sql-proxy
          chmod +x cloud-sql-proxy

      - name: Start Cloud SQL Proxy
        env:
          INSTANCE_CONNECTION_NAME: ${{ env.INSTANCE_CONNECTION_NAME }}
        run: |
          set -euo pipefail
          ./cloud-sql-proxy --port 5432 "$INSTANCE_CONNECTION_NAME" > proxy.log 2>&1 &
          for i in {1..30}; do
            (echo > /dev/tcp/127.0.0.1/5432) >/dev/null 2>&1 && break
            sleep 1
          done
          (echo > /dev/tcp/127.0.0.1/5432) >/dev/null 2>&1 || { echo "Proxy failed to start"; tail -n +1 proxy.log; exit 1; }
    
      - name: Install psql client
        run: |
          sudo apt-get update -y && sudo apt-get install -y postgresql-client

      - name: Test DB connection
        env:
          PGPASSWORD: ${{ secrets.DB_PASSWORD }}
        run: |
          psql "host=127.0.0.1 port=5432 user=${{ secrets.DB_USER }} dbname=${{ secrets.DB_NAME }} sslmode=disable" -c "select now();"

      - name: Upsert aggregate into Postgres (optional but recommended)
        env:
          PGPASSWORD: ${{ secrets.DB_PASSWORD }}
        run: |
          psql "host=127.0.0.1 port=5432 user=${{ secrets.DB_USER }} dbname=${{ secrets.DB_NAME }} sslmode=disable" -c "select now();"
          python ./backend/datapipe/upsert_moons.py --in-json build/moons_all.json

      # === Publish artifacts ===
      - name: Publish CSV to GCS and JSON to R2
        env:
          # GCS
          GCP_PROJECT_ID: ${{ secrets.GOOGLE_CLOUD_PROJECT }}
          GCS_BUCKET_NAME: ${{ secrets.GCS_BUCKET_NAME }}
          GCS_OBJECT_NAME: workflow/moons_all.csv
          # R2
          R2_ENDPOINT: ${{ secrets.R2_ENDPOINT }}
          R2_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
          R2_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
          R2_BUCKET: ${{ secrets.R2_BUCKET }}
          R2_JSON_KEY: moons_all.json
          # Also upload the per-planet outputs as “latest” if you want:
          UPLOAD_PER_PLANET: "true"
        run: |
          set -euo pipefail
          python ./backend/datapipe/publish_to_gcs.py --bucket "$GCS_BUCKET_NAME" --src build/moons_all.csv --dst "$GCS_OBJECT_NAME"
          python ./backend/datapipe/publish_to_r2.py  --endpoint "$R2_ENDPOINT" --key "$R2_ACCESS_KEY_ID" --secret "$R2_SECRET_ACCESS_KEY" --bucket "$R2_BUCKET" --src build/moons_all.json --dst "$R2_JSON_KEY"

          if [ "${UPLOAD_PER_PLANET}" = "true" ]; then
            # Optionally push per-planet “latest” aliases too:
            for f in build/moons_*.json; do
              base=$(basename "$f")
              python ./backend/datapipe/publish_to_r2.py --endpoint "$R2_ENDPOINT" --key "$R2_ACCESS_KEY_ID" --secret "$R2_SECRET_ACCESS_KEY" --bucket "$R2_BUCKET" --src "$f" --dst "$base"
            done
            for f in build/moons_*.csv; do
              base=$(basename "$f")
              python ./backend/datapipe/publish_to_gcs.py --bucket "$GCS_BUCKET_NAME" --src "$f" --dst "workflow/$base"
            done
          fi

      - name: Show Proxy Logs (on failure)
        if: failure()
        run: |
          echo "==== cloud-sql-proxy logs ===="
          cat proxy.log || true
