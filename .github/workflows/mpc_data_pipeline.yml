name: Daily Minor Planet Center (MPC) Data Pipeline

on:
  schedule:
    - cron: "0 0 * * 0"  # 00:00 UTC Sunday each week
  workflow_dispatch:

env:
  PROJECT_ID: exoatlas
  INSTANCE_CONNECTION_NAME: exoatlas:us-central1:exoatlas-db

jobs:
  fetch-and-store-data:
    runs-on: ubuntu-latest
    timeout-minutes: 90
    permissions:
      contents: 'read'
      id-token: 'write' # Required for Workload Identity Federation

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: 'Authenticate to Google Cloud'
        id: 'auth'
        uses: google-github-actions/auth@v2
        with:
          workload_identity_provider: 'projects/566178654810/locations/global/workloadIdentityPools/github-pool/providers/exoatlas'
          service_account: 'exoatlas-github-mpc-worker@exoatlas.iam.gserviceaccount.com'
          create_credentials_file: true
          export_environment_variables: true

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          pip install requests psycopg2-binary google-cloud-storage pandas

      - name: Download Cloud SQL Auth Proxy v2
        env:
          PROXY_VERSION: v2.18.1   # keep this pinned intentionally
        run: |
          wget -q https://storage.googleapis.com/cloud-sql-connectors/cloud-sql-proxy/${PROXY_VERSION}/cloud-sql-proxy.linux.amd64 -O cloud-sql-proxy
          chmod +x cloud-sql-proxy

      - name: Start Cloud SQL Proxy
        env:
          INSTANCE_CONNECTION_NAME: ${{ env.INSTANCE_CONNECTION_NAME }}
        run: |
          set -euo pipefail
          ./cloud-sql-proxy --port 5432 "$INSTANCE_CONNECTION_NAME" > proxy.log 2>&1 &
          for i in {1..30}; do
            (echo > /dev/tcp/127.0.0.1/5432) >/dev/null 2>&1 && break
            sleep 1
          done
          (echo > /dev/tcp/127.0.0.1/5432) >/dev/null 2>&1 || { echo "Proxy failed to start"; tail -n +1 proxy.log; exit 1; }
    
      - name: Install psql client
        run: |
          sudo apt-get update -y && sudo apt-get install -y postgresql-client

      - name: Test DB connection
        env:
          PGPASSWORD: ${{ secrets.DB_PASSWORD }}
        run: |
          psql "host=127.0.0.1 port=5432 user=${{ secrets.DB_USER }} dbname=${{ secrets.DB_NAME }} sslmode=disable" -c "select now();"

      - name: Run data fetcher
        env:
          # Required by your script:
          DB_USER:           ${{ secrets.DB_USER }}          
          DB_PASSWORD:       ${{ secrets.DB_PASSWORD }}    
          DB_NAME:           ${{ secrets.DB_NAME }}         
          GCS_BUCKET_NAME:   ${{ secrets.GCS_BUCKET_NAME }}  
      
          # Already set earlier by auth@v2; left here for clarity:
          GOOGLE_APPLICATION_CREDENTIALS: ${{ env.GOOGLE_GHA_CREDS_PATH }}
          
          # Test-mode knobs:
          MAX_ROWS_INGEST:   "0"
          SKIP_GCS_EXPORT:   "0"
          # (optional) make batches smaller for a snappier test:
          UPSERT_BATCH_SIZE: "500"
                
        run: |
          python backend/datapipe/mpc_data_fetcher.py
      
      # Optional: surface proxy logs if the job fails later
      - name: Show proxy logs (on failure)
        if: failure()
        run: |
          echo "==== cloud-sql-proxy logs ===="
          cat proxy.log || true
